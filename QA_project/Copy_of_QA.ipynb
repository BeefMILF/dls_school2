{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f6182e6cf1949c9b420d0bf9bba58ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62679276f25d440f99e00fef9cd01455",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9b1e1b592b934330a8725fe81d3c0b3d",
              "IPY_MODEL_5b43f2a57cc944c4a0aeba81e28615a6"
            ]
          }
        },
        "62679276f25d440f99e00fef9cd01455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b1e1b592b934330a8725fe81d3c0b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2d526578aedd4c7485277592c1a26245",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_451368f0583b4e3aa1833cc33475c2ca"
          }
        },
        "5b43f2a57cc944c4a0aeba81e28615a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab8c259f07fe4b34827b8bd9d76eb33b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 2.12kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ccbd5ea601b24b4c9da3ef39f36bd9ff"
          }
        },
        "2d526578aedd4c7485277592c1a26245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "451368f0583b4e3aa1833cc33475c2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab8c259f07fe4b34827b8bd9d76eb33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ccbd5ea601b24b4c9da3ef39f36bd9ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca13d6bb4b9845bb9bf96e131988f59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8fd398b1b2a94aa5b314eae525295615",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2fe800bb6e94d5a8f783c629a98b327",
              "IPY_MODEL_e583fe3b6efa4862877aab6d4156017e"
            ]
          }
        },
        "8fd398b1b2a94aa5b314eae525295615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2fe800bb6e94d5a8f783c629a98b327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba644bf9aa334c63af2969030969b8e4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82fbbc4f7e564cb197d0a1e9e8672837"
          }
        },
        "e583fe3b6efa4862877aab6d4156017e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3456950c7a74453aab7cb81dbc022b55",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.35MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a16afb58ee464faebf521caeb1c32f38"
          }
        },
        "ba644bf9aa334c63af2969030969b8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82fbbc4f7e564cb197d0a1e9e8672837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3456950c7a74453aab7cb81dbc022b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a16afb58ee464faebf521caeb1c32f38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3757eae24e3f40a1b52b75bd9f15aaee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6122f6be3a8447f29d833e7a438895df",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6e473bc3d2d452c8268ab1ad0461fe1",
              "IPY_MODEL_4b9aab2a31d84b18816e7f1f5d038f73"
            ]
          }
        },
        "6122f6be3a8447f29d833e7a438895df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6e473bc3d2d452c8268ab1ad0461fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d56d49c42ac54d4e8e40c023bae47270",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad5bd837423b4c568e951fd9fbc4dd39"
          }
        },
        "4b9aab2a31d84b18816e7f1f5d038f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_af5fe87970ee48d38a5baf884679e85f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 3.41MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f4950ccdea24b3d9c8d64189faed807"
          }
        },
        "d56d49c42ac54d4e8e40c023bae47270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad5bd837423b4c568e951fd9fbc4dd39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af5fe87970ee48d38a5baf884679e85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f4950ccdea24b3d9c8d64189faed807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4cda58046028453686fee5db60842530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_171a232d80dd4921908504085d42aa79",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1ca2dc1787e34552b4a349a5c2bc3895",
              "IPY_MODEL_7be5ca2edf85499c934446ba703bb1d8"
            ]
          }
        },
        "171a232d80dd4921908504085d42aa79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ca2dc1787e34552b4a349a5c2bc3895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_51457c6e29cc4343ae7410837a664575",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bed9e87637d44e318ec40446f87cb2c9"
          }
        },
        "7be5ca2edf85499c934446ba703bb1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d385460764843588ca9b3e9e43f52f5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9427/? [00:10&lt;00:00, 889.02it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0054539c13284de6bf3cf251255f2bc5"
          }
        },
        "51457c6e29cc4343ae7410837a664575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bed9e87637d44e318ec40446f87cb2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d385460764843588ca9b3e9e43f52f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0054539c13284de6bf3cf251255f2bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e90503e04fc346a0a2f07d77cebd0e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9363769bc0b747c681398c852827487f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0883e12fe47a4be3b12c744ec3f480ec",
              "IPY_MODEL_1262e85f0c0e4a09911521e93edfc63b"
            ]
          }
        },
        "9363769bc0b747c681398c852827487f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0883e12fe47a4be3b12c744ec3f480ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f98628d129a4e12bdb7013499333d47",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2698b1354b7a43a6aa341062a67f318b"
          }
        },
        "1262e85f0c0e4a09911521e93edfc63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7e0cca755e7b41ffaf8b204bb8210e27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3270/? [00:02&lt;00:00, 1112.63it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f5923be320248718111e1ff6402738e"
          }
        },
        "4f98628d129a4e12bdb7013499333d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2698b1354b7a43a6aa341062a67f318b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e0cca755e7b41ffaf8b204bb8210e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f5923be320248718111e1ff6402738e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "344711d5accd4b17a1510d016d069b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65a4a4836e8f482989a541541a36b905",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_71d86ef0ad7e457780dba0818bc6da72",
              "IPY_MODEL_73b7d5bfd72f449e86e92ff0455c6200"
            ]
          }
        },
        "65a4a4836e8f482989a541541a36b905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "71d86ef0ad7e457780dba0818bc6da72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ff83fdfd0304b68a878db3ba796cdcb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19ab1bdb12204cc791ac928cd34fc11c"
          }
        },
        "73b7d5bfd72f449e86e92ff0455c6200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_be1b1f8904ea4d85872311489b520e4c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:11&lt;00:00, 43.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ab90a25b9b44ff4a2da3ec4541db841"
          }
        },
        "2ff83fdfd0304b68a878db3ba796cdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19ab1bdb12204cc791ac928cd34fc11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be1b1f8904ea4d85872311489b520e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ab90a25b9b44ff4a2da3ec4541db841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeefMILF/dls_school2/blob/master/QA_project/Copy_of_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd06vTCzqj3y",
        "colab_type": "text"
      },
      "source": [
        "# Проект QA\n",
        "\n",
        "## Yes/No Questions\n",
        "\n",
        "Вы будете работать с корпусом BoolQ. Корпус состоит из вопросов, предполагающих бинарный ответ (да / нет), абзацев из Википедии,  содержащих ответ на вопрос, заголовка статьи, из которой извлечен абзац и непосредственно ответа (true / false).\n",
        "\n",
        "Корпус описан в статье:\n",
        "\n",
        "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova\n",
        "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\n",
        "\n",
        "https://arxiv.org/abs/1905.10044\n",
        "\n",
        "\n",
        "Корпус (train-dev split) доступен в репозитории проекта:  https://github.com/google-research-datasets/boolean-questions\n",
        "\n",
        "Используйте для обучения train часть корпуса, для валидации и тестирования – dev часть. \n",
        "\n",
        "Каждый бонус пункт оцениватся в 1 балл. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOup-Ca-q5Ay",
        "colab_type": "code",
        "outputId": "152d89cd-69a2-4be0-e7f4-50814b62e0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!gsutil cp gs://boolq/train.jsonl .\n",
        "!gsutil cp gs://boolq/dev.jsonl ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://boolq/train.jsonl...\n",
            "/ [1 files][  6.2 MiB/  6.2 MiB]                                                \n",
            "Operation completed over 1 objects/6.2 MiB.                                      \n",
            "Copying gs://boolq/dev.jsonl...\n",
            "/ [1 files][  2.1 MiB/  2.1 MiB]                                                \n",
            "Operation completed over 1 objects/2.1 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDEsCCkoqj34",
        "colab_type": "text"
      },
      "source": [
        "### Пример вопроса: \n",
        "question: is batman and robin a sequel to batman forever\n",
        "\n",
        "title: Batman & Robin (film)\n",
        "\n",
        "answer: true\n",
        "\n",
        "passage: With the box office success of Batman Forever in June 1995, Warner Bros. immediately commissioned a sequel. They hired director Joel Schumacher and writer Akiva Goldsman to reprise their duties the following August, and decided it was best to fast track production for a June 1997 target release date, which is a break from the usual 3-year gap between films. Schumacher wanted to homage both the broad camp style of the 1960s television series and the work of Dick Sprang. The storyline of Batman & Robin was conceived by Schumacher and Goldsman during pre-production on A Time to Kill. Portions of Mr. Freeze's back-story were based on the Batman: The Animated Series episode ''Heart of Ice'', written by Paul Dini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5CnuYdqj3_",
        "colab_type": "text"
      },
      "source": [
        "## Часть 1. Эксплоративный анализ\n",
        "1. Посчитайте долю yes и no классов в корпусе\n",
        "2. Оцените среднюю длину вопроса\n",
        "3. Оцените среднюю длину параграфа\n",
        "4. Предположите, по каким эвристикам были собраны вопросы (или найдите ответ в статье). Продемонстриуйте, как эти эвристики повлияли на структуру корпуса. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRTuUReAqj4D",
        "colab_type": "text"
      },
      "source": [
        "## Часть 2. Baseline\n",
        "1. Оцените accuracy точность совсем простого базового решения: присвоить каждой паре вопрос-ответ в dev части самый частый класс из train части\n",
        "2. Оцените accuracy чуть более сложного базового решения: fasttext на текстах, состоящих из склееных вопросов и абзацев (' '.join([question, passage]))\n",
        "\n",
        "Почему fasttext плохо справляется с этой задачей?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg84O0bfqj4G",
        "colab_type": "text"
      },
      "source": [
        "## Часть 3. Используем эмбеддинги предложений\n",
        "1. Постройте BERT эмбеддинги вопроса и абзаца. Обучите логистическую регрессию на конкатенированных эмбеддингах вопроса и абзаца и оцените accuracy этого решения. \n",
        "\n",
        "[bonus] Используйте другие модели эмбеддингов, доступные, например, в библиотеке 🤗 Transformers. Какая модель эмбеддингов даст лучшие результаты?\n",
        "\n",
        "[bonus] Предложите метод аугментации данных и продемонстрируйте его эффективность. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghUVREQjqj4H",
        "colab_type": "text"
      },
      "source": [
        "## Часть 3. DrQA-подобная архитектура\n",
        "\n",
        "Основана на статье: Reading Wikipedia to Answer Open-Domain Questions\n",
        "\n",
        "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes\n",
        "\n",
        "https://arxiv.org/abs/1704.00051\n",
        "\n",
        "Архитектура DrQA предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
        "1. Кодировщик абзаца [paragraph encoding] – LSTM, получаящая на вход вектора слов, состоящие из: \n",
        "* эмбеддинга слова (w2v или fasttext)\n",
        "* дополнительных признаков-индикаторов, кодирующих в виде one-hot векторов часть речи слова, является ли оно именованной сущностью или нет, встречается ли слово в вопросе или нет \n",
        "* выровненного эмбеддинга вопроса, получаемого с использованием soft attention между эмбеддингами слов из абзаца и эмбеддингом вопроса.\n",
        "\n",
        "$f_{align}(p_i) = \\sum_j􏰂 a_{i,j} E(q_j)$, где $E(q_j)$ – эмбеддинг слова из вопроса. Формула для $a_{i,j}$ приведена в статье. \n",
        "\n",
        "2. Кодировщик вопроса [question encoding] – LSTM, получаящая на вход эмбеддинги слов из вопроса. Выход кодировщика: $q = 􏰂\\sum_j􏰂  b_j q_j$. Формула для $b_{j}$ приведена в статье. \n",
        "\n",
        "3. Слой предсказания. \n",
        "\n",
        "Предложите, как можно было модифицировать последний слой предсказания в архитектуре DrQA, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
        "\n",
        "Оцените качество этой модели для решения задачи. \n",
        "\n",
        "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXmluyTFqj4P",
        "colab_type": "text"
      },
      "source": [
        "## Часть 4. BiDAF-подобная архитектура\n",
        "\n",
        "Основана на статье: Bidirectional Attention Flow for Machine Comprehension\n",
        "\n",
        "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
        "\n",
        "https://arxiv.org/abs/1611.01603\n",
        "\n",
        "Архитектура BiDAF предложена для задачи SQuAD, но легко может быть адаптирована к текущему заданию. Модель состоит из следующих блоков:\n",
        "1. Кодировщик  получает на вход два представления слова: эмбеддинг слова и полученное из CNN посимвольное представление слова. Кодировщики для вопроса и для параграфа одинаковы. \n",
        "2. Слой внимания (детальное описание приведено в статье, см. пункт Attention Flow Layer)\n",
        "3. Промежуточный слой, который получает на вход контекстуализированные эмбеддинги слов из параграфа, состоящие из трех частей (выход кодировщика параграфа,   Query2Context (один вектор) и Context2Query (матрица) выравнивания\n",
        "\n",
        "4. Слой предсказания. \n",
        "\n",
        "Предложите, как можно было модифицировать последний слой предсказания в архитектуре BiDAF, с учетом того, что итоговое предсказание – это метка yes / no, предсказание которой проще, чем предсказание спана ответа для SQuAD.\n",
        "\n",
        "Оцените качество этой модели для решения задачи. \n",
        "\n",
        "[bonus] Замените входные эмбеддинги и все дополнительные признаки, используемые кодировщиками, на BERT эмбеддинги. Улучшит ли это качество результатов?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrPAxeHIqj4S",
        "colab_type": "text"
      },
      "source": [
        "Сравнение DrQA и BiDAF:\n",
        "    \n",
        "![](https://www.researchgate.net/profile/Felix_Wu6/publication/321069852/figure/fig1/AS:560800147881984@1510716582560/Schematic-layouts-of-the-BiDAF-left-and-DrQA-right-architectures-We-propose-to.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMXmEGfgqj4W",
        "colab_type": "text"
      },
      "source": [
        "## Часть 5. Итоги\n",
        "Напишите краткое резюме проделанной работы. Сравните результаты всех разработанных моделей. Что помогло вам в выполнении работы, чего не хватало?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxddM6I_rbpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze3C_gnGZX0N",
        "colab_type": "text"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib3aeW7rZdZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://boolq/train.jsonl .\n",
        "!gsutil cp gs://boolq/dev.jsonl ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2zJ2fFi5iAu",
        "colab_type": "text"
      },
      "source": [
        "### Installation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjelRV3NFkjT",
        "colab_type": "text"
      },
      "source": [
        "***transformers***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uddN2n765hEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "9da34bbf-5843-4931-8427-e0b3886b5d7e"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eacNpgFtmv",
        "colab_type": "text"
      },
      "source": [
        "***fasttext***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN1DHOEeFXwu",
        "colab_type": "code",
        "outputId": "6c047dc1-ddc7-4b50-a9ec-b61bd6cd5113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fastText' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0ppMH_5FYvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac46e33d-1fb9-4021-d5d4-df2a0b81fbd0"
      },
      "source": [
        "# cd fastText"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fastText\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X30_cYFEFf_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmRi17a3Fjrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6744638-f327-49b8-91fa-e2bcb46dc199"
      },
      "source": [
        "# cd .. "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_h7tYICMDRQ",
        "colab_type": "text"
      },
      "source": [
        "***catalyst***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4k_qCpVMBGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -U catalyst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AljAK1LECOjU",
        "colab_type": "text"
      },
      "source": [
        "***FP16***, float16 format for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7YVfWVWCN30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd3726d3-4291-4d15-b1ec-87c5cd395754"
      },
      "source": [
        "# if Your machine doesn't support FP16, comment these 4 lines below\n",
        "!git clone https://github.com/NVIDIA/apex \n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex \n",
        "!rm -rf ./apex\n",
        "FP16_PARAMS = dict(opt_level=\"O1\") \n",
        "# FP16_PARAMS = None"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 7255, done.\u001b[K\n",
            "remote: Total 7255 (delta 0), reused 0 (delta 0), pack-reused 7255\u001b[K\n",
            "Receiving objects: 100% (7255/7255), 13.85 MiB | 23.26 MiB/s, done.\n",
            "Resolving deltas: 100% (4901/4901), done.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-5wemcabt\n",
            "Created temporary directory: /tmp/pip-req-tracker-pnjbbsqb\n",
            "Created requirements tracker '/tmp/pip-req-tracker-pnjbbsqb'\n",
            "Created temporary directory: /tmp/pip-install-1rw4ik8h\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-vrif970l\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-pnjbbsqb'\n",
            "    Running setup.py (path:/tmp/pip-req-build-vrif970l/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-vrif970l/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-vrif970l/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-vrif970l has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-pnjbbsqb'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Created temporary directory: /tmp/pip-uninstall-38jc2mri\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex-0.1-py3.6.egg-info\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "      Created temporary directory: /usr/local/lib/python3.6/dist-packages/~pex\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex/\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "      Removing file or directory /usr/local/lib/python3.6/dist-packages/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "      Successfully uninstalled apex-0.1\n",
            "  Created temporary directory: /tmp/pip-record-v4pek0tg\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-vrif970l/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-vrif970l/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-v4pek0tg/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-vrif970l/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                             \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_60,code=sm_60 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-v4pek0tg/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-vrif970l\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-pnjbbsqb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xP0FNARIQW",
        "colab_type": "text"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FAHrUK_RASN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c43619ab-520b-406e-a642-7c05e966a9ac"
      },
      "source": [
        "import random \n",
        "\n",
        "# Numpy & Pandas \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Matplotlib & Seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "sns.set()\n",
        "\n",
        "# Sklearn \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# PyTorch \n",
        "import torch\n",
        "from torch import nn \n",
        "from torch.nn import functional as F \n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# FastText\n",
        "import fasttext\n",
        "\n",
        "# Transformers \n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Catalyst\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import AccuracyCallback, F1ScoreCallback, OptimizerCallback\n",
        "from catalyst.dl.callbacks import CheckpointCallback, InferCallback\n",
        "from catalyst.utils import set_global_seed, prepare_cudnn\n",
        "from catalyst.dl import utils\n",
        "from catalyst.data import BalanceClassSampler"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MfETs36RNJl",
        "colab_type": "text"
      },
      "source": [
        "### Read json "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMS0iMXkRA5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b1be209e-c2ef-4f29-f5bc-a6599802bcf2"
      },
      "source": [
        "# Loading data\n",
        "df_train = pd.read_json(\"/content/train.jsonl\", lines=True, orient='records')\n",
        "df_dev = pd.read_json(\"/content/dev.jsonl\", lines=True, orient=\"records\")\n",
        "\n",
        "print(f'Train df size: {df_train.shape}')\n",
        "print(f'Dev df size: {df_dev.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train df size: (9427, 4)\n",
            "Dev df size: (3270, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19e4n4BcRWaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "4c34bcde-d7ba-4491-c7d5-85715dcdaec7"
      },
      "source": [
        "df_train.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>title</th>\n",
              "      <th>answer</th>\n",
              "      <th>passage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>do iran and afghanistan speak the same language</td>\n",
              "      <td>Persian language</td>\n",
              "      <td>True</td>\n",
              "      <td>Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>do good samaritan laws protect those who help ...</td>\n",
              "      <td>Good Samaritan law</td>\n",
              "      <td>True</td>\n",
              "      <td>Good Samaritan laws offer legal protection to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  ...                                            passage\n",
              "0    do iran and afghanistan speak the same language  ...  Persian (/ˈpɜːrʒən, -ʃən/), also known by its ...\n",
              "1  do good samaritan laws protect those who help ...  ...  Good Samaritan laws offer legal protection to ...\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCUm0MPNRQCV",
        "colab_type": "text"
      },
      "source": [
        "## Task 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77xVP7jQSgJK",
        "colab_type": "code",
        "outputId": "37624091-9dfa-4579-e7fa-522a744f3ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
        "sns.countplot(x='answer', data=df_train, ax=ax[0])\n",
        "sns.countplot(x='answer', data=df_dev, ax=ax[1])\n",
        "\n",
        "# switch off ylabel \n",
        "ax[1].set_ylabel('')    \n",
        "\n",
        "ax[0].set_title('Train $\\'answer\\'$ distribution')\n",
        "ax[1].set_title('Dev $\\'answer\\'$ distribution')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KCyw0_qS4eq",
        "colab_type": "code",
        "outputId": "224f1327-1bd2-44dd-cc7b-db6e7a034b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "qlen_train_mean = int(df_train.question.str.len().mean())\n",
        "qlen_dev_mean = int(df_dev.question.str.len().mean())\n",
        "\n",
        "f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot a simple histogram with binsize determined automatically\n",
        "ax1 = sns.distplot(df_train.question.str.len(), kde=False, color=\"b\", label=f'mean len {qlen_train_mean}', ax=axes[0])\n",
        "ax2 = sns.distplot(df_dev.question.str.len(), kde=False, color=\"b\", label=f'mean len {qlen_dev_mean}', ax=axes[1])\n",
        "\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "\n",
        "f.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQWEX7ekVQ6k",
        "colab_type": "code",
        "outputId": "0174c8c4-8827-4322-9816-f72d4ae85992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "qlen_train_mean = int(df_train.passage.str.len().mean())\n",
        "qlen_dev_mean = int(df_dev.passage.str.len().mean())\n",
        "\n",
        "f, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot a simple histogram with binsize determined automatically\n",
        "ax1 = sns.distplot(df_train.passage.str.len(), kde=False, color=\"b\", label=f'mean len {qlen_train_mean}', ax=axes[0])\n",
        "ax2 = sns.distplot(df_dev.passage.str.len(), kde=False, color=\"b\", label=f'mean len {qlen_dev_mean}', ax=axes[1])\n",
        "\n",
        "ax1.legend()\n",
        "ax2.legend()\n",
        "\n",
        "f.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoRd1lxgZbxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrD4uZmEauHd",
        "colab_type": "text"
      },
      "source": [
        "## Task 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz-Efj3_bY4d",
        "colab_type": "text"
      },
      "source": [
        "### Baseline model, predicting the most frequent class \n",
        "\n",
        "Train accuracy: <mark>0.6231</mark>\n",
        "\n",
        "Dev accuracy: <mark>0.6217</mark>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AaZpSnzawDi",
        "colab_type": "code",
        "outputId": "45956970-1bb9-49e1-a2fb-2ca659a7032d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "base_answer = df_train.answer.value_counts().idxmax()\n",
        "\n",
        "train_preds = np.repeat(base_answer, len(df_train))\n",
        "dev_preds = np.repeat(base_answer, len(df_dev))\n",
        "\n",
        "print(f'Train accuracy: {accuracy_score(df_train.answer, train_preds):.4f}')\n",
        "print(f'Dev accuracy: {accuracy_score(df_dev.answer, dev_preds):.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.6231\n",
            "Dev accuracy: 0.6217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcGYGnNadcv4",
        "colab_type": "text"
      },
      "source": [
        "### Fasttext model\n",
        "\n",
        "***Resourses:***\n",
        "\n",
        "* [fasttext python usage](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
        "* [fasttext with command line](https://gosha20777.github.io/tutorial/2018/04/12/fasttext-for-windows/)\n",
        "\n",
        "\n",
        "\n",
        "* *Model 1.* `wordNgrams=2, epoch=15`\n",
        "\n",
        "Train accuracy: <mark>0.9275</mark>\n",
        "\n",
        "Dev accuracy: <mark>0.6853</mark>\n",
        "\n",
        "* *Model 2.* `wordNgrams=3, epoch=10, lr=0.2, dim=50`\n",
        "\n",
        "Train accuracy: <mark>0.9567</mark>\n",
        "\n",
        "Dev accuracy: <mark>0.6945</mark>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtjUR9QUgO83",
        "colab_type": "code",
        "outputId": "c7e6f28d-c559-4593-bcec-e86b6ea43469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_tgt = '__label__' + df_train.answer.astype(str)\n",
        "dev_tgt = '__label__' + df_dev.answer.astype(str)\n",
        "\n",
        "train_data = df_train[['question', 'passage']].apply(lambda r: ' '.join(r), axis=1)\n",
        "dev_data = df_dev[['question', 'passage']].apply(lambda r: ' '.join(r), axis=1)\n",
        "\n",
        "print(f'Train total lines: {len(train_data)}')\n",
        "print(f'Dev total lines: {len(dev_data)}')\n",
        "\n",
        "np.savetxt('train.txt', (train_tgt + ' ' + train_data).values, delimiter=\" \", fmt=\"%s\") \n",
        "np.savetxt('dev.txt', dev_data, delimiter=\" \", fmt=\"%s\") \n",
        "\n",
        "train_tgt = train_tgt.values \n",
        "dev_tgt = dev_tgt.values\n",
        "\n",
        "train_data = train_data.values\n",
        "dev_data = dev_data.values "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train total lines: 9427\n",
            "Dev total lines: 3270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y6sf78VkYsl",
        "colab_type": "code",
        "outputId": "718bea3f-ac17-4326-bff1-07ec36ee9649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def fit_fasttext(path, train_data, dev_data, train_tgt, dev_tgt, **kwargs): \n",
        "    # model without parameters performed ~ 0.67 on dev data \n",
        "    model = fasttext.train_supervised(input=\"train.txt\", **kwargs)\n",
        "\n",
        "    train_preds = np.array([model.predict(row)[0][0] for row in train_data]) \n",
        "    dev_preds = np.array([model.predict(row)[0][0] for row in dev_data]) \n",
        "\n",
        "    print(f'Train accuracy: {accuracy_score(train_preds, train_tgt):.4f}')\n",
        "    print(f'Dev accuracy: {accuracy_score(dev_preds, dev_tgt):.4f}')\n",
        "\n",
        "\n",
        "# first variant \n",
        "fit_fasttext('train.txt', train_data, dev_data, train_tgt, dev_tgt, \n",
        "             wordNgrams=2, epoch=15)\n",
        "\n",
        "print() \n",
        "# second variant \n",
        "fit_fasttext('train.txt', train_data, dev_data, train_tgt, dev_tgt, \n",
        "             wordNgrams=3, epoch=10, lr=0.2, dim=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.9275\n",
            "Dev accuracy: 0.6853\n",
            "\n",
            "Train accuracy: 0.9567\n",
            "Dev accuracy: 0.6945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVX9md_2xrjh",
        "colab_type": "text"
      },
      "source": [
        "## Task 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTK3qox4Tvz",
        "colab_type": "text"
      },
      "source": [
        "### Data encoding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grfsGkbHxu1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f6182e6cf1949c9b420d0bf9bba58ad",
            "62679276f25d440f99e00fef9cd01455",
            "9b1e1b592b934330a8725fe81d3c0b3d",
            "5b43f2a57cc944c4a0aeba81e28615a6",
            "2d526578aedd4c7485277592c1a26245",
            "451368f0583b4e3aa1833cc33475c2ca",
            "ab8c259f07fe4b34827b8bd9d76eb33b",
            "ccbd5ea601b24b4c9da3ef39f36bd9ff",
            "ca13d6bb4b9845bb9bf96e131988f59a",
            "8fd398b1b2a94aa5b314eae525295615",
            "a2fe800bb6e94d5a8f783c629a98b327",
            "e583fe3b6efa4862877aab6d4156017e",
            "ba644bf9aa334c63af2969030969b8e4",
            "82fbbc4f7e564cb197d0a1e9e8672837",
            "3456950c7a74453aab7cb81dbc022b55",
            "a16afb58ee464faebf521caeb1c32f38",
            "3757eae24e3f40a1b52b75bd9f15aaee",
            "6122f6be3a8447f29d833e7a438895df",
            "b6e473bc3d2d452c8268ab1ad0461fe1",
            "4b9aab2a31d84b18816e7f1f5d038f73",
            "d56d49c42ac54d4e8e40c023bae47270",
            "ad5bd837423b4c568e951fd9fbc4dd39",
            "af5fe87970ee48d38a5baf884679e85f",
            "9f4950ccdea24b3d9c8d64189faed807",
            "4cda58046028453686fee5db60842530",
            "171a232d80dd4921908504085d42aa79",
            "1ca2dc1787e34552b4a349a5c2bc3895",
            "7be5ca2edf85499c934446ba703bb1d8",
            "51457c6e29cc4343ae7410837a664575",
            "bed9e87637d44e318ec40446f87cb2c9",
            "8d385460764843588ca9b3e9e43f52f5",
            "0054539c13284de6bf3cf251255f2bc5",
            "e90503e04fc346a0a2f07d77cebd0e0b",
            "9363769bc0b747c681398c852827487f",
            "0883e12fe47a4be3b12c744ec3f480ec",
            "1262e85f0c0e4a09911521e93edfc63b",
            "4f98628d129a4e12bdb7013499333d47",
            "2698b1354b7a43a6aa341062a67f318b",
            "7e0cca755e7b41ffaf8b204bb8210e27",
            "7f5923be320248718111e1ff6402738e"
          ]
        },
        "outputId": "8893c163-b11d-47bc-880f-c9af3ef41a6d"
      },
      "source": [
        "def encode_data(tokenizer, questions, passages, max_length):\n",
        "    \"\"\"Encode the question/passage pairs into features than can be fed to the model.\"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for question, passage in tqdm(zip(questions, passages)):\n",
        "        encoded_data = tokenizer.encode_plus(question, passage, max_length=max_length, pad_to_max_length=True, truncation_strategy=\"longest_first\")\n",
        "        encoded_pair = encoded_data[\"input_ids\"]\n",
        "        attention_mask = encoded_data[\"attention_mask\"]\n",
        "\n",
        "        input_ids.append(encoded_pair)\n",
        "        attention_masks.append(attention_mask)\n",
        "\n",
        "    return np.array(input_ids), np.array(attention_masks)\n",
        "\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "random.seed(33)\n",
        "np.random.seed(33)\n",
        "torch.manual_seed(33)\n",
        "\n",
        "# Pretrained tokenizer \n",
        "pretrained_model_name = 'roberta-base'\n",
        "# pretrained_model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name) \n",
        "\n",
        "\n",
        "passages_train = df_train.passage.values\n",
        "questions_train = df_train.question.values\n",
        "answers_train = df_train.answer.values.astype(int)\n",
        "\n",
        "passages_dev = df_dev.passage.values\n",
        "questions_dev = df_dev.question.values\n",
        "answers_dev = df_dev.answer.values.astype(int)\n",
        "\n",
        "# Encoding data\n",
        "max_seq_length = 256\n",
        "\n",
        "input_ids_train, attention_masks_train = encode_data(tokenizer, questions_train, passages_train, max_seq_length)\n",
        "input_ids_dev, attention_masks_dev = encode_data(tokenizer, questions_dev, passages_dev, max_seq_length)\n",
        "\n",
        "train_features = (input_ids_train, attention_masks_train, answers_train)\n",
        "dev_features = (input_ids_dev, attention_masks_dev, answers_dev)\n",
        "\n",
        "print(f'Train params size,  input ids: {input_ids_train.shape}, attention masks: {attention_masks_train.shape} , answers(targets): {answers_train.shape}')\n",
        "print(f'Dev params size,  input ids: {input_ids_dev.shape}, attention masks: {attention_masks_dev.shape} , answers(targets): {answers_dev.shape}')\n",
        "\n",
        "# Split Dev into test/val \n",
        "test_val_inds = np.random.choice(2, len(answers_dev), p=[0.3, 0.7]).astype(bool)\n",
        "test_features = [feature[~test_val_inds] for feature in dev_features]\n",
        "val_features =  [feature[test_val_inds] for feature in dev_features]\n",
        "\n",
        "print(f'Test params size,  input ids: {test_features[0].shape}, attention masks: {test_features[1].shape} , answers(targets): {test_features[2].shape}')\n",
        "print(f'Validation params size,  input ids: {val_features[0].shape}, attention masks: {val_features[1].shape} , answers(targets): {val_features[2].shape}')\n",
        "\n",
        "print('Sample: ')\n",
        "input_ids_train[0], attention_masks_train[0], answers_train[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f6182e6cf1949c9b420d0bf9bba58ad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca13d6bb4b9845bb9bf96e131988f59a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3757eae24e3f40a1b52b75bd9f15aaee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cda58046028453686fee5db60842530",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e90503e04fc346a0a2f07d77cebd0e0b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train params size,  input ids: (9427, 256), attention masks: (9427, 256) , answers(targets): (9427,)\n",
            "Dev params size,  input ids: (3270, 256), attention masks: (3270, 256) , answers(targets): (3270,)\n",
            "Test params size,  input ids: (994, 256), attention masks: (994, 256) , answers(targets): (994,)\n",
            "Validation params size,  input ids: (2276, 256), attention masks: (2276, 256) , answers(targets): (2276,)\n",
            "Sample: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([    0,   109, 10209,   260,     8,  9724,  5158,  7566,  1994,\n",
              "            5,   276,  2777,     2,     2, 27775, 48229, 49066,   642,\n",
              "        35423,    48, 43621, 16948,   338, 38155, 10659, 35423,    27,\n",
              "          282,     6,   111, 38155,   862, 35423,    27,   282,    73,\n",
              "          238,    67,   684,    30,    63,   253, 41433,   274,  2726,\n",
              "          118,    36, 26068, 10172, 29438, 29434, 40637, 44148, 14285,\n",
              "          856, 18195,  4926,   118,    36,   506, 35423, 10659, 43621,\n",
              "        16948, 35423,  4726, 49066, 11000, 43621, 16948,    43,    36,\n",
              "         4161, 46934,    16,    65,     9,     5,  2027,  5051, 11991,\n",
              "          624,     5, 27617,    12, 21336,   811,  6084,     9,     5,\n",
              "        27617,    12, 17108,  2777,   284,     4,    85,    16,  4212,\n",
              "         5826,    11,  1603,     6,  4035,    36, 27257, 11416,   684,\n",
              "           25,   211,  1512,   187, 23102,   238,     8, 17335,   967,\n",
              "         7566,    36, 27257, 11416,   684,    25, 17335,  8907,   187,\n",
              "            5,  8297,  3567,   238,     8,   103,    97,  3806,    61,\n",
              "         9644,    58, 27775,   877, 17537,     8,  1687,   233,     9,\n",
              "         9312,  1603,     4,    85,    16,  1982,    11,     5, 27775,\n",
              "        34555,     6,    10, 10639, 17390,     9,     5, 19645,  8543,\n",
              "            6,    61,  1495, 12236,    31,     5, 21918, 20620, 34555,\n",
              "            4,     2,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "            1,     1,     1,     1]),\n",
              " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrQ7CYI6DnIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Same data encoding, but works longer  \n",
        "\n",
        "# enc_data = df_train[['question', 'passage']].apply(lambda x: get_attrs(tokenizer.encode_plus(*x.values, max_length=256, pad_to_max_length=True, truncation_strategy=\"longest_first\")), axis=1)\n",
        "# train_features = (enc_data.apply(lambda x: x[0]).values, enc_data.apply(lambda x: x[1]).values, df_train.answer.values.astype(int))\n",
        "\n",
        "# enc_data = df_dev[['question', 'passage']].apply(lambda x: get_attrs(tokenizer.encode_plus(*x.values, max_length=256, pad_to_max_length=True, truncation_strategy=\"longest_first\")), axis=1)\n",
        "# train_features = (enc_data.apply(lambda x: x[0]).values, enc_data.apply(lambda x: x[1]).values, df_dev.answer.values.astype(int))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWJK0XwyNPdI",
        "colab_type": "text"
      },
      "source": [
        "### Prepare loaders "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3y00fEVZwBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e79dbf1f-5bbc-4cc1-c2c5-e6e1367f9806"
      },
      "source": [
        "class BQDataset(Dataset): \n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        assert len(data) == 3\n",
        "\n",
        "        self.ids, self.attn_masks, self.tgt = data\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, ind): \n",
        "        return {'features': self.ids[ind], \n",
        "                'attention_mask': self.attn_masks[ind], \n",
        "                'targets': self.tgt[ind]}\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in train_features]\n",
        "val_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in val_features]\n",
        "test_features_tensors = [torch.tensor(feature, dtype=torch.long) for feature in test_features]\n",
        "\n",
        "# difference between Dataset and TensorDataset - https://mc.ai/tutorial-creating-a-dataset-in-pytorch/\n",
        "\n",
        "# No way to forward as dict \n",
        "# train_dataset = TensorDataset(*train_features_tensors)\n",
        "# val_dataset = TensorDataset(*val_features_tensors)\n",
        "# test_dataset = TensorDataset(*test_features_tensors)\n",
        "\n",
        "train_dataset = BQDataset(train_features_tensors)\n",
        "val_dataset = BQDataset(val_features_tensors)\n",
        "test_dataset = BQDataset(test_features_tensors)\n",
        "\n",
        "# train_sampler = RandomSampler(train_dataset)\n",
        "train_sampler = BalanceClassSampler(answers_train, mode=\"upsampling\")\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "loaders = {\n",
        "    'train': train_dataloader, \n",
        "    'valid': val_dataloader,\n",
        "    'test': test_dataloader,     \n",
        "} \n",
        "\n",
        "print(f'Train loader size: {len(loaders[\"train\"])}')\n",
        "print(f'Validation loader size: {len(loaders[\"valid\"])}')\n",
        "print(f'Test loader size: {len(loaders[\"test\"])}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loader size: 1469\n",
            "Validation loader size: 285\n",
            "Test loader size: 125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt_i4D7vkbt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cls_counts = pd.value_counts(answers_train).values \n",
        "# weights = 1 / torch.tensor(cls_counts).double()\n",
        "# train_sampler = WeightedRandomSampler(weights[answers_train], num_samples=len(answers_train))\n",
        "# train_sampler = RandomSampler(train_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmn20Gp_7Fi",
        "colab_type": "code",
        "outputId": "490de8f9-0cc3-415b-dd12-59f01c2eb379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        }
      },
      "source": [
        "def plot_loader_dist(loader): \n",
        "    cls0, cls1 = [], []\n",
        "    for b in train_dataloader:  \n",
        "        item = pd.value_counts(b['targets'].detach().cpu().numpy()).values\n",
        "        if len(item) == 2: \n",
        "            cls0.append(item[0]), cls1.append(item[1])\n",
        "         \n",
        "\n",
        "    sns.distplot(cls0, kde=False, color=\"b\", label=f'cls0, mean {np.mean(cls0):.3f}')\n",
        "    sns.distplot(cls1, kde=False, color=\"r\", label=f'cls1, mean {np.mean(cls1):.3f}')\n",
        "    plt.legend()\n",
        "    plt.show() \n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# 1. Bad case\n",
        "cls_counts = pd.value_counts(answers_train).values \n",
        "weights = 1 / torch.tensor(cls_counts).double()\n",
        "\n",
        "train_sampler = WeightedRandomSampler(weights[answers_train], num_samples=len(answers_train))\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "plt.title('Imbalanced sampler')\n",
        "plot_loader_dist(train_dataloader)\n",
        "\n",
        "\n",
        "# 2. Upsampling mode\n",
        "train_sampler = BalanceClassSampler(answers_train, mode=\"upsampling\")\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "plt.title('Balanced upsampler')\n",
        "plot_loader_dist(train_dataloader)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1do_r88y4rE",
        "colab_type": "code",
        "outputId": "7634dc79-d9b3-4780-e44b-5fef64af9b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "next(iter(train_dataloader))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " 'features': tensor([[  101,  2003,  1996,  ...,     0,     0,     0],\n",
              "         [  101,  2003, 10090,  ...,     0,     0,     0],\n",
              "         [  101,  2003,  2343,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  2003,  9044,  ...,     0,     0,     0],\n",
              "         [  101,  2515,  2697,  ...,     0,     0,     0],\n",
              "         [  101,  2515,  5984,  ...,     0,     0,     0]]),\n",
              " 'targets': tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
              "         1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
              "         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-58YHtAVX0n",
        "colab_type": "code",
        "outputId": "9924cb23-ec4d-437c-f44b-411f1900029c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "344711d5accd4b17a1510d016d069b29",
            "65a4a4836e8f482989a541541a36b905",
            "71d86ef0ad7e457780dba0818bc6da72",
            "73b7d5bfd72f449e86e92ff0455c6200",
            "2ff83fdfd0304b68a878db3ba796cdcb",
            "19ab1bdb12204cc791ac928cd34fc11c",
            "be1b1f8904ea4d85872311489b520e4c",
            "2ab90a25b9b44ff4a2da3ec4541db841"
          ]
        }
      },
      "source": [
        "class Model(nn.Module): \n",
        "    def __init__(self, pretrained_model_name: str):\n",
        "        super().__init__() \n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name)\n",
        "\n",
        "    def n_trainable(self): \n",
        "        return sum([params.numel() for name, params in self.model.named_parameters() if params.requires_grad])\n",
        "\n",
        "    def forward(self, features, attention_mask, targets):\n",
        "        output = self.model(input_ids=features, \n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=targets)\n",
        "        logits = output[1]\n",
        "        return logits \n",
        "\n",
        "\n",
        "model = Model(pretrained_model_name)\n",
        "model.to(device)\n",
        "print(f'Trainable parameters: {model.n_trainable()}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "344711d5accd4b17a1510d016d069b29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Trainable parameters: 125237762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztwm4dki6Blg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from catalyst.dl import Callback, CallbackOrder, Runner\n",
        "\n",
        "\n",
        "class F1ScoreCallback(Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_key: str = \"targets\",\n",
        "        output_key: str = \"logits\",\n",
        "        activation: str = \"Sigmoid\",\n",
        "        prefix: str = \"f1_score\",\n",
        "    ):\n",
        "\n",
        "        self.prefix = prefix\n",
        "        self.input_key = input_key\n",
        "        self.output_key = output_key\n",
        "\n",
        "        super().__init__(CallbackOrder.Metric)\n",
        "\n",
        "    def on_batch_end(self, state: Runner):\n",
        "        y_true = state.input[self.input_key].detach().cpu().numpy()\n",
        "        logits = state.output[self.output_key].detach().cpu().numpy()\n",
        "        y_pred = logits.argmax(1)\n",
        "\n",
        "        score = f1_score(y_true, y_pred)\n",
        "\n",
        "        state.batch_metrics.update(\n",
        "            {self.prefix: score}\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNxUofQUaI4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "508bc233-7bab-4d52-80ab-9a1af7d295bb"
      },
      "source": [
        "set_global_seed(33)                       # reproducibility\n",
        "prepare_cudnn(deterministic=True)           # reproducibility\n",
        "\n",
        "LOG_DIR = './logs'\n",
        "epochs = 6\n",
        "lr = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
        "acum_step = 4\n",
        "num_cls = 2 \n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "runner = SupervisedRunner(\n",
        "    input_key=(\n",
        "        'features',\n",
        "        'attention_mask', \n",
        "        'targets'\n",
        "    ),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=loaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=num_cls),\n",
        "        F1ScoreCallback(),\n",
        "        OptimizerCallback(\n",
        "            accumulation_steps=acum_step, \n",
        "            grad_clip_params={'func': 'clip_grad_value_', 'clip_value': 1}\n",
        "            )\n",
        "    ],\n",
        "    main_metric=\"accuracy01\",\n",
        "    minimize_metric=False,\n",
        "    # fp16=FP16_PARAMS,\n",
        "    logdir=LOG_DIR,\n",
        "    num_epochs=epochs,\n",
        "    verbose=False, \n",
        "    load_best_on_end=True\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O0:  Pure FP32 training.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "[2020-06-13 15:13:19,205] \n",
            "1/6 * Epoch 1 (_base): lr=4.050e-05 | momentum=0.9000\n",
            "1/6 * Epoch 1 (train): accuracy01=0.5766 | f1_score=0.5345 | loss=0.6666\n",
            "1/6 * Epoch 1 (valid): accuracy01=0.6287 | f1_score=0.6537 | loss=0.6518\n",
            "1/6 * Epoch 1 (test): accuracy01=0.6549 | f1_score=0.6610 | loss=0.6358\n",
            "[2020-06-13 15:13:19,205] \n",
            "1/6 * Epoch 1 (_base): lr=4.050e-05 | momentum=0.9000\n",
            "1/6 * Epoch 1 (train): accuracy01=0.5766 | f1_score=0.5345 | loss=0.6666\n",
            "1/6 * Epoch 1 (valid): accuracy01=0.6287 | f1_score=0.6537 | loss=0.6518\n",
            "1/6 * Epoch 1 (test): accuracy01=0.6549 | f1_score=0.6610 | loss=0.6358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning:\n",
            "\n",
            "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-13 15:20:31,941] \n",
            "2/6 * Epoch 2 (_base): lr=3.645e-05 | momentum=0.9000\n",
            "2/6 * Epoch 2 (train): accuracy01=0.7524 | f1_score=0.7320 | loss=0.5140\n",
            "2/6 * Epoch 2 (valid): accuracy01=0.6916 | f1_score=0.7299 | loss=0.5996\n",
            "2/6 * Epoch 2 (test): accuracy01=0.7123 | f1_score=0.7439 | loss=0.5813\n",
            "[2020-06-13 15:20:31,941] \n",
            "2/6 * Epoch 2 (_base): lr=3.645e-05 | momentum=0.9000\n",
            "2/6 * Epoch 2 (train): accuracy01=0.7524 | f1_score=0.7320 | loss=0.5140\n",
            "2/6 * Epoch 2 (valid): accuracy01=0.6916 | f1_score=0.7299 | loss=0.5996\n",
            "2/6 * Epoch 2 (test): accuracy01=0.7123 | f1_score=0.7439 | loss=0.5813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-13 15:27:42,858] \n",
            "3/6 * Epoch 3 (_base): lr=3.281e-05 | momentum=0.9000\n",
            "3/6 * Epoch 3 (train): accuracy01=0.8825 | f1_score=0.8653 | loss=0.2989\n",
            "3/6 * Epoch 3 (valid): accuracy01=0.6687 | f1_score=0.6908 | loss=0.8564\n",
            "3/6 * Epoch 3 (test): accuracy01=0.7022 | f1_score=0.7055 | loss=0.7741\n",
            "[2020-06-13 15:27:42,858] \n",
            "3/6 * Epoch 3 (_base): lr=3.281e-05 | momentum=0.9000\n",
            "3/6 * Epoch 3 (train): accuracy01=0.8825 | f1_score=0.8653 | loss=0.2989\n",
            "3/6 * Epoch 3 (valid): accuracy01=0.6687 | f1_score=0.6908 | loss=0.8564\n",
            "3/6 * Epoch 3 (test): accuracy01=0.7022 | f1_score=0.7055 | loss=0.7741\n",
            "[2020-06-13 15:34:11,559] \n",
            "4/6 * Epoch 4 (_base): lr=2.952e-05 | momentum=0.9000\n",
            "4/6 * Epoch 4 (train): accuracy01=0.9434 | f1_score=0.9351 | loss=0.1643\n",
            "4/6 * Epoch 4 (valid): accuracy01=0.7078 | f1_score=0.7391 | loss=0.8782\n",
            "4/6 * Epoch 4 (test): accuracy01=0.7103 | f1_score=0.7334 | loss=0.8321\n",
            "[2020-06-13 15:34:11,559] \n",
            "4/6 * Epoch 4 (_base): lr=2.952e-05 | momentum=0.9000\n",
            "4/6 * Epoch 4 (train): accuracy01=0.9434 | f1_score=0.9351 | loss=0.1643\n",
            "4/6 * Epoch 4 (valid): accuracy01=0.7078 | f1_score=0.7391 | loss=0.8782\n",
            "4/6 * Epoch 4 (test): accuracy01=0.7103 | f1_score=0.7334 | loss=0.8321\n",
            "[2020-06-13 15:41:22,901] \n",
            "5/6 * Epoch 5 (_base): lr=2.657e-05 | momentum=0.9000\n",
            "5/6 * Epoch 5 (train): accuracy01=0.9677 | f1_score=0.9585 | loss=0.0966\n",
            "5/6 * Epoch 5 (valid): accuracy01=0.7276 | f1_score=0.7706 | loss=1.2019\n",
            "5/6 * Epoch 5 (test): accuracy01=0.7465 | f1_score=0.7869 | loss=1.0851\n",
            "[2020-06-13 15:41:22,901] \n",
            "5/6 * Epoch 5 (_base): lr=2.657e-05 | momentum=0.9000\n",
            "5/6 * Epoch 5 (train): accuracy01=0.9677 | f1_score=0.9585 | loss=0.0966\n",
            "5/6 * Epoch 5 (valid): accuracy01=0.7276 | f1_score=0.7706 | loss=1.2019\n",
            "5/6 * Epoch 5 (test): accuracy01=0.7465 | f1_score=0.7869 | loss=1.0851\n",
            "[2020-06-13 15:48:34,997] \n",
            "6/6 * Epoch 6 (_base): lr=2.391e-05 | momentum=0.9000\n",
            "6/6 * Epoch 6 (train): accuracy01=0.9782 | f1_score=0.9722 | loss=0.0672\n",
            "6/6 * Epoch 6 (valid): accuracy01=0.7223 | f1_score=0.7655 | loss=1.0987\n",
            "6/6 * Epoch 6 (test): accuracy01=0.7294 | f1_score=0.7674 | loss=1.0113\n",
            "[2020-06-13 15:48:34,997] \n",
            "6/6 * Epoch 6 (_base): lr=2.391e-05 | momentum=0.9000\n",
            "6/6 * Epoch 6 (train): accuracy01=0.9782 | f1_score=0.9722 | loss=0.0672\n",
            "6/6 * Epoch 6 (valid): accuracy01=0.7223 | f1_score=0.7655 | loss=1.0987\n",
            "6/6 * Epoch 6 (test): accuracy01=0.7294 | f1_score=0.7674 | loss=1.0113\n",
            "Top best models:\n",
            "logs/checkpoints/train.5.pth\t0.7276\n",
            "=> Loading checkpoint logs/checkpoints/best_full.pth\n",
            "loaded state checkpoint logs/checkpoints/best_full.pth (global epoch 5, epoch 5, stage train)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj9dtomjlaJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "4b271406-2e41-4237-9556-e368ab6785b8"
      },
      "source": [
        "set_global_seed(33)                       # reproducibility\n",
        "prepare_cudnn(deterministic=True)           # reproducibility\n",
        "\n",
        "LOG_DIR = './logs'\n",
        "epochs = 6\n",
        "lr = 5e-5                      # learning rate is typically ~1e-5 for transformers\n",
        "acum_step = 4\n",
        "num_cls = 2 \n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
        "\n",
        "runner = SupervisedRunner(\n",
        "    input_key=(\n",
        "        'features',\n",
        "        'attention_mask', \n",
        "        'targets'\n",
        "    ),\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=loaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=num_cls),\n",
        "        F1ScoreCallback(),\n",
        "        OptimizerCallback(\n",
        "            accumulation_steps=acum_step, \n",
        "            grad_clip_params={'func': 'clip_grad_value_', 'clip_value': 1}\n",
        "            )\n",
        "    ],\n",
        "    main_metric=\"accuracy01\",\n",
        "    minimize_metric=False,\n",
        "    # fp16=FP16_PARAMS,\n",
        "    logdir=LOG_DIR,\n",
        "    num_epochs=epochs,\n",
        "    verbose=False, \n",
        "    load_best_on_end=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O0:  Pure FP32 training.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning:\n",
            "\n",
            "F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-13 16:54:46,860] \n",
            "1/6 * Epoch 1 (_base): lr=4.050e-05 | momentum=0.9000\n",
            "1/6 * Epoch 1 (train): accuracy01=0.5815 | f1_score=0.5069 | loss=0.6718\n",
            "1/6 * Epoch 1 (valid): accuracy01=0.6705 | f1_score=0.7038 | loss=0.6051\n",
            "1/6 * Epoch 1 (test): accuracy01=0.7022 | f1_score=0.7338 | loss=0.5956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning:\n",
            "\n",
            "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-13 17:02:20,022] \n",
            "2/6 * Epoch 2 (_base): lr=3.645e-05 | momentum=0.9000\n",
            "2/6 * Epoch 2 (train): accuracy01=0.7188 | f1_score=0.6798 | loss=0.5643\n",
            "2/6 * Epoch 2 (valid): accuracy01=0.6920 | f1_score=0.7005 | loss=0.5791\n",
            "2/6 * Epoch 2 (test): accuracy01=0.6952 | f1_score=0.6994 | loss=0.5783\n",
            "[2020-06-13 17:09:48,879] \n",
            "3/6 * Epoch 3 (_base): lr=3.281e-05 | momentum=0.9000\n",
            "3/6 * Epoch 3 (train): accuracy01=0.8174 | f1_score=0.7938 | loss=0.4249\n",
            "3/6 * Epoch 3 (valid): accuracy01=0.7412 | f1_score=0.7656 | loss=0.5872\n",
            "3/6 * Epoch 3 (test): accuracy01=0.7414 | f1_score=0.7622 | loss=0.5694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4YoKqauNl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYOiyXOeuMHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir $LOG_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwyzUZoE2hqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4of_Hhq2iLR",
        "colab_type": "code",
        "outputId": "a3ea5f6c-cbe4-41a7-eedb-6df64cb023c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "runner.infer(\n",
        "    model=model,\n",
        "    loaders={'test': test_dataloader},\n",
        "    callbacks=[\n",
        "        CheckpointCallback(\n",
        "            resume=f\"{LOG_DIR}/checkpoints/best.pth\"\n",
        "        ),\n",
        "        InferCallback(),\n",
        "    ],   \n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O0:  Pure FP32 training.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O0\n",
            "cast_model_type        : torch.float32\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "=> Loading checkpoint ./logs/checkpoints/best.pth\n",
            "loaded state checkpoint ./logs/checkpoints/best.pth (global epoch 5, epoch 5, stage train)\n",
            "1/1 * Epoch (test): 100% 125/125 [00:08<00:00, 15.07it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U1AxlpuJJnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_probs = runner.callbacks[0].predictions['logits']\n",
        "\n",
        "y_preds = predicted_probs.argmax(1)\n",
        "y_true = np.array(list(map(lambda x: x['targets'], test_dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBJ05fJg6Zux",
        "colab_type": "code",
        "outputId": "2295d59b-3c34-462e-9f25-9a3a0fba370b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "accuracy_score(y_true, y_preds)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7464788732394366"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9oT0yrOHMY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FcFX2S3HMbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K42Qjk_X9XRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inds0 = y_preds == 0\n",
        "inds1 = y_preds == 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2kMGbM69rRZ",
        "colab_type": "code",
        "outputId": "f066b194-3f3e-46e5-d44d-feda072fb102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(y_true[inds0], y_preds[inds0]), accuracy_score(y_true[inds1], y_preds[inds1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6909090909090909, 0.7740963855421686)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks5iCchSIr27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}